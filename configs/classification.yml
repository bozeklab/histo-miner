
# All path and names related parameters 
  
  paths:                                           # --- parent key, DO NOT FILL this line

    folders:                                       # --- parent key, DO NOT FILL this line

      save_trained_model:                          # Path to the folder to save trained classification model [str]

      inference_input:                             # Path to the folder containing input WSI for classiciation inference [str]
 
    files:                                         # --- parent key, DO NOT FILL this line

      feature_selection_file:                      # Path to the feature selection file, for either pre-defined or custom feature selection [str]                               

  names:                                           # --- parent key, DO NOT FILL this line

    run_name: "run_1"                              # Name of the training run. Default: "run_1"  [str]

    trained_model: "model_1"                       # Name of the trained model: Default: "model_1"  [str]

# All none strings parameters 

  parameters:                                      # --- parent key, DO NOT FILL this line

    bool:                                          # --- parent key, DO NOT FILL this line

      predefined_feature_selection: True           # Choose to use the same selected features as is in the paper (True) or to run a custom feature selection (False). Default: True [bool] 

      display_classification_predictions: True     # Display or not the classification prediction in the terminal while running classification inference. Default: True [bool]                         

      display_classification_scores: True          # Display or not the classification scores in the terminal while running classification inference. Default: True [bool]

      run_classifiers:                             # --- parent key, DO NOT FILL this line

        xgboost: True                              # If True: choose xgboost as classifier model for training and inference. Then light_gbm needs to be set on False. Default: True [bool] 

        light_gbm: False                           # If True: choose light gbm as classifier model for training and inference. Then xgboost needs to be set on False. Default: False [bool]                         

    int:                                           # --- parent key, DO NOT FILL this line

      nbr_of_splits: 3                             # Number of spits used to perform cross validation. Default: 3 [int]

      nestedcross_inner_splits:                    # Number of nested splits used to perform nested cross validation (not used in the paper) [int]

# Classifiers parameters in case of grid search (not performed in paper analyses) - Then no docstring added

  classifierparam:
    
    ridge:
      
      random_state: 0

      alpha:  0

      grid_dict: 

          random_state: [0, 42, 84]

          alpha: [0, 0.1, 0.5, 1, 2, 4, 8, 16]

    logistic_regression:

      random_state: 0

      penalty: 'l2'

      solver: 'liblinear'

      multi_class: 'ovr'

      class_weight: 'balanced'

      grid_dict: 

          random_state: [0, 42, 84]

          penalty: ['l1', 'l2']

          solver: ['liblinear']

          multi_class: ['ovr']

          class_weight: ['balanced']

    random_forest:

      random_state: 84

      n_estimators: 10

      class_weight: 'balanced'

      grid_dict: 
          random_state:  [0, 42, 84]

          n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]

          class_weight: ['balanced']

    xgboost:
      
      random_state: 0

      n_estimators: 100

      learning_rate:  0.3

      objective: 'binary:logistic'

      grid_dict:

          random_state: [0, 42]

          n_estimators: [20, 40, 60, 80, 100, 120, 160, 200]

          learning_rate: [0.001, 0.01, 0.1, 0.8, 1]

          objective: ['binary:logistic']

    light_gbm:
      
      random_state: 42

      n_estimators: 100

      learning_rate: 0.1

      objective: 'binary'

      num_leaves: 31

      grid_dict: 

            random_state: [0, 42, 84]

            n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]

            learning_rate: [0.001, 0.01, 0.1, 0.8, 1, 1.2] 

            objective: ['binary']

            num_leaves: [2, 5, 10, 15, 20, 40]
 

# Boruta parameters

  borutaparam:
    boruta_max_depth: 1 

    boruta_random_state: 0



# high depth for Boruta can prevent feature selection and return no feature
# low depth can be detrimental and lead to too many features selected 

# For more information on the parameters check sklearn.linear_model.RidgeClassifier documentation
# #https://scikit-learn.org/stable/modules/linear_model.html

# For more information on the parameters check sklearn.linear_model.LogisticRegression documentation
# #https://scikit-learn.org/stable/modules/linear_model.html

# For more information on the parameters check sklearn.linear_model.RandomForestClassifier documentation
# #https://scikit-learn.org/stable/modules/linear_model.html

# For more information on xgboost parameters check the documentation here:
# #https://xgboost.readthedocs.io/en/stable/parameter.html
# and
# #https://xgboost.readthedocs.io/en/stable/get_started.html

# For more information on light_gbm parameters check the documentation here:
# #https://lightgbm.readthedocs.io/en/stable/Parameters.html
# and
# #https://lightgbm.readthedocs.io/en/stable/Python-Intro.html

