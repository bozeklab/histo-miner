
# Could be just classifiation the name of file becasue I think it is not 
# used only for training (to check)


# All parameters of string names
  names:
    # run_name: "nestedcross_rec_directmunich"
    run_name: "preprint_CPI_80_0-1"

    # permutation_idx:

    #   perm_bestsplit: "bestperm/random_permutation_index_11_28_xgboost_bestsplit"

    #   perm_cvmean: "bestperm/random_permutation_index_11_28_xgboost_bestmean"



# All not strings parameters
  parameters:
    bool:

      classification_from_allfeatures: True

      run_classifiers:
        ridge: False

        logistic_regression: False

        random_forest: False

        xgboost: True   

        light_gbm: False                                     

      save_evaluations: True

      search_bestsplit: False


    int:
      nbr_of_splits: 5

      nestedcross_inner_splits: 3


# Classifiers parameters

  classifierparam:
    ridge:
      # For more information on the parameters check sklearn.linear_model.RidgeClassifier documentation
      # #https://scikit-learn.org/stable/modules/linear_model.html
      
      random_state: 0

      alpha:  0
      # for best split: 0.1
      # for best mean: 0

      grid_dict: 
          random_state: [0, 42, 84]
          # random_state: [0, 42, 84]
          # for dev: random_state: [0]

          alpha: [0, 0.1, 0.5, 1, 2, 4, 8, 16]
          # alpha: [0, 0.1, 0.5, 1, 2, 4, 8, 16]
          # for dev: [0, 8]

    logistic_regression:
      # For more information on the parameters check sklearn.linear_model.LogisticRegression documentation
      # #https://scikit-learn.org/stable/modules/linear_model.html
      random_state: 0

      penalty: 'l2'

      solver: 'liblinear'
      # choice for binary classification

      multi_class: 'ovr'
      # choice for binary classification

      class_weight: 'balanced'

      grid_dict: 
          random_state: [0, 42, 84]
          # random_state: [0, 42, 84]
          # for dev: [0]

          penalty: ['l1', 'l2']
          # penalty: ['l1', 'l2']
          # for dev: ['l1', 'l2']

          solver: ['liblinear']

          multi_class: ['ovr']

          class_weight: ['balanced']


    random_forest:
      # For more information on the parameters check sklearn.linear_model.RandomForestClassifier documentation
      # #https://scikit-learn.org/stable/modules/linear_model.html
      
      random_state: 84
      # For best split 84

      n_estimators: 10
      # The number of trees in the forest.
      # For best split: 100

      class_weight: 'balanced'

      grid_dict: 
          random_state:  [0, 42, 84]
          # random_state: [0, 42, 84]
          # for dev: [0, 84]

          n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]
          # n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]
          # for dev: [60, 180]

          class_weight: ['balanced']

    xgboost:
      # For more information on xgboost parameters check the documentation here:
      # #https://xgboost.readthedocs.io/en/stable/parameter.html
      # and
      # #https://xgboost.readthedocs.io/en/stable/get_started.html
      
      random_state: 0

      n_estimators: 80
      # number of boosting iterations, should be = number of trees in the forest
      # for best split: 40
      # for best mean: 40

      learning_rate:  0.1
      # for best split: 0.1
      # for best mean: 0.1

      objective: 'binary:logistic'
      # choice to take for binary classification

      grid_dict:
          random_state: [0, 42]
          # random_state: [0, 42, 84]
          # for dev: [0]

          n_estimators: [20, 40, 60, 80, 100, 120, 160, 200]
          # n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]
          # for dev: [20, 100]
         
          learning_rate: [0.001, 0.01, 0.1, 0.8, 1]
          # learning_rate: [0.001, 0.01, 0.1, 0.8, 1, 1.2]  
          # for dev: [0.1, 1.2]

          objective: ['binary:logistic']

    light_gbm:
      # For more information on light_gbm parameters check the documentation here:
      # #https://lightgbm.readthedocs.io/en/stable/Parameters.html
      # and
      # #https://lightgbm.readthedocs.io/en/stable/Python-Intro.html
      
      random_state: 42

      n_estimators: 200
      # number of boosting iterations, should be = number of trees in the forest
      # for best split: 100
      # for best mean: 120
      # for best mean, slides selected: 60

      learning_rate: 0.01
      # for best split: 0.8
      # for best mean: 0.01
      # for best mean, slides selected:  0.1

      objective: 'binary'
      # choice to take for binary classification

      num_leaves: 2
      # for best split: 10
      # for best mean: 20
      # for best mean, slides selected: 15

      grid_dict: 
            random_state: [0, 42, 84]
            # random_state: [0, 42, 84]
            # for dev: [0]

            n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]
            # n_estimators: [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]
            # for dev: [40, 100]

            learning_rate: [0.001, 0.01, 0.1, 0.8, 1, 1.2] 
            # learning_rate: [0.001, 0.01, 0.1, 0.8, 1, 1.2] 
            # for dev: [0.8, 1.2]

            objective: ['binary']

            num_leaves: [2, 5, 10, 15, 20, 40]
            # num_leaves: [2, 5, 10, 15, 20, 40]
            # for dev: [10]
 
















